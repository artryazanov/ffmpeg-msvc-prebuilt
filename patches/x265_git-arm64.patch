diff --git a/source/CMakeLists.txt b/source/CMakeLists.txt
index 9f93b6e..65884e9 100755
--- a/source/CMakeLists.txt
+++ b/source/CMakeLists.txt
@@ -100,7 +100,11 @@ elseif(ARM64MATCH GREATER "-1")
     option(ENABLE_SVE2_BITPERM "Enable SVE2 BitPerm" ON)
 
     # Compiler flags for AArch64 extensions.
-    set(AARCH64_NEON_FLAG "-march=armv8-a")
+    if(NOT MSVC)
+        set(AARCH64_NEON_FLAG "-march=armv8-a")
+    else()
+        set(AARCH64_NEON_FLAG "")
+    endif()
     # Neon DotProd is mandatory from Armv8.4.
     set(AARCH64_NEON_DOTPROD_FLAG "-march=armv8.2-a+dotprod")
     # Neon I8MM is mandatory from Armv8.6.
diff --git a/source/common/aarch64/arm64-utils.cpp b/source/common/aarch64/arm64-utils.cpp
index af93729..44f8a30 100644
--- a/source/common/aarch64/arm64-utils.cpp
+++ b/source/common/aarch64/arm64-utils.cpp
@@ -246,7 +246,7 @@ void transpose32x32(uint8_t *dst, const uint8_t *src, intptr_t dstride, intptr_t
     transpose16x16(dst + 16 * dstride + 16, src + 16 * sstride + 16, dstride, sstride);
     if (dst == src)
     {
-        uint8_t tmp[16 * 16] __attribute__((aligned(64)));
+        ALIGN_VAR_64(uint8_t, tmp[16 * 16]);
         transpose16x16(tmp, src + 16, 16, sstride);
         transpose16x16(dst + 16, src + 16 * sstride, dstride, sstride);
         for (int i = 0; i < 16; i++)
@@ -372,7 +372,7 @@ void transpose32x32(uint16_t *dst, const uint16_t *src, intptr_t dstride, intptr
         {
             if (dst == src)
             {
-                uint16_t tmp[8 * 8] __attribute__((aligned(64)));
+                ALIGN_VAR_64(uint16_t, tmp[8 * 8]);
                 transpose8x8(tmp, src + 8 * i + 8 * j * sstride, 8, sstride);
                 transpose8x8(dst + 8 * i + 8 * j * dstride, src + 8 * j + 8 * i * sstride, dstride, sstride);
                 for (int k = 0; k < 8; k++)
diff --git a/source/common/aarch64/intrapred-prim.cpp b/source/common/aarch64/intrapred-prim.cpp
index 3d4b476..5180b4d 100644
--- a/source/common/aarch64/intrapred-prim.cpp
+++ b/source/common/aarch64/intrapred-prim.cpp
@@ -2,7 +2,7 @@
 #include "primitives.h"
 
 
-#if HAVE_NEON
+#if HAVE_NEON || defined(_M_ARM64)
 #include "arm64-utils.h"
 #include "mem-neon.h"
 #include <arm_neon.h>
@@ -541,34 +541,35 @@ static void dcPredFilter(const pixel* above, const pixel* left, pixel* dst, intp
     case 16:
     case 8:
     {
+#if HIGH_BIT_DEPTH
         uint16x8_t vconst_3 = vdupq_n_u16(3);
         uint16x8_t vconst_2 = vdupq_n_u16(2);
         for (int x = 0; x < size; x += 8) {
-            uint16x8_t vabo = { (uint16_t)(above[x + 0]),
-                                (uint16_t)(above[x + 1]),
-                                (uint16_t)(above[x + 2]),
-                                (uint16_t)(above[x + 3]),
-                                (uint16_t)(above[x + 4]),
-                                (uint16_t)(above[x + 5]),
-                                (uint16_t)(above[x + 6]),
-                                (uint16_t)(above[x + 7]) };
-
-            uint16x8_t vdst = { (uint16_t)(dst[x + 0]),
-                                (uint16_t)(dst[x + 1]),
-                                (uint16_t)(dst[x + 2]),
-                                (uint16_t)(dst[x + 3]),
-                                (uint16_t)(dst[x + 4]),
-                                (uint16_t)(dst[x + 5]),
-                                (uint16_t)(dst[x + 6]),
-                                (uint16_t)(dst[x + 7]) };
-            //  dst[x] = (pixel)((above[x] +  3 * dst[x] + 2) >> 2);
+            uint16x8_t vabo = vld1q_u16(above + x);
+            uint16x8_t vdst = vld1q_u16(dst + x);
+
+            vdst = vmulq_u16(vdst, vconst_3);
+            vdst = vaddq_u16(vdst, vabo);
+            vdst = vaddq_u16(vdst, vconst_2);
+            vdst = vshrq_n_u16(vdst, 2);
+
+            vst1q_u16(dst + x, vdst);
+        }
+#else
+        uint16x8_t vconst_3 = vdupq_n_u16(3);
+        uint16x8_t vconst_2 = vdupq_n_u16(2);
+        for (int x = 0; x < size; x += 8) {
+            uint16x8_t vabo = vmovl_u8(vld1_u8((const uint8_t*)(above + x)));
+            uint16x8_t vdst = vmovl_u8(vld1_u8((uint8_t*)(dst + x)));
+
             vdst = vmulq_u16(vdst, vconst_3);
             vdst = vaddq_u16(vdst, vabo);
             vdst = vaddq_u16(vdst, vconst_2);
             vdst = vshrq_n_u16(vdst, 2);
-            for (int i = 0; i < 8; i++)
-                dst[x + i] = (pixel)(vdst[i]);
+
+            vst1_u8((uint8_t*)(dst + x), vmovn_u16(vdst));
         }
+#endif
         dst += dststride;
         for (int y = 1; y < size; y++)
         {
@@ -579,22 +580,37 @@ static void dcPredFilter(const pixel* above, const pixel* left, pixel* dst, intp
     break;
     case 4:
     {
+#if HIGH_BIT_DEPTH
+        uint16x4_t vconst_3 = vdup_n_u16(3);
+        uint16x4_t vconst_2 = vdup_n_u16(2);
+        uint16x4_t vabo = vld1_u16(above);
+        uint16x4_t vdstx = vld1_u16(dst);
+
+        vdstx = vmul_u16(vdstx, vconst_3);
+        vdstx = vadd_u16(vdstx, vabo);
+        vdstx = vadd_u16(vdstx, vconst_2);
+        vdstx = vshr_n_u16(vdstx, 2);
+        vst1_u16(dst, vdstx);
+#else
         uint16x4_t vconst_3 = vdup_n_u16(3);
         uint16x4_t vconst_2 = vdup_n_u16(2);
-        uint16x4_t vabo = { (uint16_t)(above[0]),
-                            (uint16_t)(above[1]),
-                            (uint16_t)(above[2]),
-                            (uint16_t)(above[3]) };
-        uint16x4_t vdstx = { (uint16_t)(dst[0]),
-                             (uint16_t)(dst[1]),
-                             (uint16_t)(dst[2]),
-                             (uint16_t)(dst[3]) };
+
+        uint8x8_t vabo_u8 = vdup_n_u8(0);
+        vabo_u8 = vreinterpret_u8_u32(vld1_lane_u32((const uint32_t*)above, vreinterpret_u32_u8(vabo_u8), 0));
+        uint16x4_t vabo = vget_low_u16(vmovl_u8(vabo_u8));
+
+        uint8x8_t vdst_u8 = vdup_n_u8(0);
+        vdst_u8 = vreinterpret_u8_u32(vld1_lane_u32((const uint32_t*)dst, vreinterpret_u32_u8(vdst_u8), 0));
+        uint16x4_t vdstx = vget_low_u16(vmovl_u8(vdst_u8));
+
         vdstx = vmul_u16(vdstx, vconst_3);
         vdstx = vadd_u16(vdstx, vabo);
         vdstx = vadd_u16(vdstx, vconst_2);
         vdstx = vshr_n_u16(vdstx, 2);
-        for (int i = 0; i < 4; i++)
-            dst[i] = (pixel)(vdstx[i]);
+
+        uint16x8_t vdst_comb = vcombine_u16(vdstx, vdup_n_u16(0));
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(vmovn_u16(vdst_comb)), 0);
+#endif
 
         dst += dststride;
         for (int y = 1; y < size; y++)
@@ -621,22 +637,13 @@ void intra_pred_dc_neon(pixel* dst, intptr_t dstStride, const pixel* srcPix, int
     case 8:
     {
         for (int i = 0; i < width; i += 8) {
-            uint16x8_t spa = { (uint16_t)(srcPix[i + 1]),
-                               (uint16_t)(srcPix[i + 2]),
-                               (uint16_t)(srcPix[i + 3]),
-                               (uint16_t)(srcPix[i + 4]),
-                               (uint16_t)(srcPix[i + 5]),
-                               (uint16_t)(srcPix[i + 6]),
-                               (uint16_t)(srcPix[i + 7]),
-                               (uint16_t)(srcPix[i + 8]) };
-            uint16x8_t spb = { (uint16_t)(srcPix[2 * width + i + 1]),
-                               (uint16_t)(srcPix[2 * width + i + 2]),
-                               (uint16_t)(srcPix[2 * width + i + 3]),
-                               (uint16_t)(srcPix[2 * width + i + 4]),
-                               (uint16_t)(srcPix[2 * width + i + 5]),
-                               (uint16_t)(srcPix[2 * width + i + 6]),
-                               (uint16_t)(srcPix[2 * width + i + 7]),
-                               (uint16_t)(srcPix[2 * width + i + 8]) };
+#if HIGH_BIT_DEPTH
+            uint16x8_t spa = vld1q_u16(srcPix + i + 1);
+            uint16x8_t spb = vld1q_u16(srcPix + 2 * width + i + 1);
+#else
+            uint16x8_t spa = vmovl_u8(vld1_u8((const uint8_t*)(srcPix + i + 1)));
+            uint16x8_t spb = vmovl_u8(vld1_u8((const uint8_t*)(srcPix + 2 * width + i + 1)));
+#endif
             uint16x8_t vsp = vaddq_u16(spa, spb);
             dcVal += vaddlvq_u16(vsp);
         }
@@ -645,27 +652,40 @@ void intra_pred_dc_neon(pixel* dst, intptr_t dstStride, const pixel* srcPix, int
         for (k = 0; k < width; k++)
             for (l = 0; l < width; l += 8) {
                 uint16x8_t vdv = vdupq_n_u16((pixel)dcVal);
-                for (int n = 0; n < 8; n++)
-                    dst[k * dstStride + l + n] = (pixel)(vdv[n]);
+#if HIGH_BIT_DEPTH
+                vst1q_u16(dst + k * dstStride + l, vdv);
+#else
+                vst1_u8((uint8_t*)(dst + k * dstStride + l), vmovn_u16(vdv));
+#endif
             }
     }
     break;
     case 4:
     {
-        uint16x4_t spa = { (uint16_t)(srcPix[1]), (uint16_t)(srcPix[2]),
-                           (uint16_t)(srcPix[3]), (uint16_t)(srcPix[4]) };
-        uint16x4_t spb = { (uint16_t)(srcPix[2 * width + 1]),
-                           (uint16_t)(srcPix[2 * width + 2]),
-                           (uint16_t)(srcPix[2 * width + 3]),
-                           (uint16_t)(srcPix[2 * width + 4]) };
+#if HIGH_BIT_DEPTH
+        uint16x4_t spa = vld1_u16(srcPix + 1);
+        uint16x4_t spb = vld1_u16(srcPix + 2 * width + 1);
+#else
+        uint8x8_t spa_u8 = vdup_n_u8(0);
+        spa_u8 = vreinterpret_u8_u32(vld1_lane_u32((const uint32_t*)(srcPix + 1), vreinterpret_u32_u8(spa_u8), 0));
+        uint16x4_t spa = vget_low_u16(vmovl_u8(spa_u8));
+
+        uint8x8_t spb_u8 = vdup_n_u8(0);
+        spb_u8 = vreinterpret_u8_u32(vld1_lane_u32((const uint32_t*)(srcPix + 2 * width + 1), vreinterpret_u32_u8(spb_u8), 0));
+        uint16x4_t spb = vget_low_u16(vmovl_u8(spb_u8));
+#endif
         uint16x4_t vsp = vadd_u16(spa, spb);
         dcVal += vaddlv_u16(vsp);
 
         dcVal = dcVal / (width + width);
         for (k = 0; k < width; k++) {
             uint16x4_t vdv = vdup_n_u16((pixel)dcVal);
-            for (int n = 0; n < 4; n++)
-                dst[k * dstStride + n] = (pixel)(vdv[n]);
+#if HIGH_BIT_DEPTH
+            vst1_u16(dst + k * dstStride, vdv);
+#else
+            uint16x8_t vdv_comb = vcombine_u16(vdv, vdup_n_u16(0));
+            vst1_lane_u32((uint32_t*)(dst + k * dstStride), vreinterpret_u32_u8(vmovn_u16(vdv_comb)), 0);
+#endif
         }
     }
     break;
diff --git a/source/common/aarch64/intrapred-prim.h b/source/common/aarch64/intrapred-prim.h
index 31da91d..9ee057b 100644
--- a/source/common/aarch64/intrapred-prim.h
+++ b/source/common/aarch64/intrapred-prim.h
@@ -1,6 +1,6 @@
 #ifndef INTRAPRED_PRIM_H__
 
-#if defined(__aarch64__)
+#if defined(__aarch64__) || defined(_M_ARM64)
 
 namespace X265_NS
 {
